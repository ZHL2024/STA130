{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60683504",
   "metadata": {},
   "source": [
    "## Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269bbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "titanic_data = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798c2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = titanic_data.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eab006",
   "metadata": {},
   "source": [
    "## Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9f1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 891\n",
      "Number of columns: 12\n"
     ]
    }
   ],
   "source": [
    "rows, columns = titanic_data.shape\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918b71a",
   "metadata": {},
   "source": [
    "### My own general definitions of the meaning of \"obervations\" and \"variables\":\n",
    "\n",
    "#### 1. Oberservations: It refers to some data we will focus on analyzing that contains a bunch of more detailed information.\n",
    "\n",
    "#### 2. Variables: It refers to data with detailed information, such as the features and characteristics that make up an observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f8810",
   "metadata": {},
   "source": [
    "## Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a3cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "summary = titanic_data.describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3991530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "Embarked\n",
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: count, dtype: int64\n",
      "Pclass\n",
      "3    491\n",
      "1    216\n",
      "2    184\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(titanic_data['Sex'].value_counts())\n",
    "print(titanic_data['Embarked'].value_counts())\n",
    "print(titanic_data['Pclass'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c37174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_per_column = titanic_data.isnull().sum()\n",
    "print(missing_values_per_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "421c6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0.0\n",
      "Survived         0.0\n",
      "Pclass           0.0\n",
      "Age            177.0\n",
      "SibSp            0.0\n",
      "Parch            0.0\n",
      "Fare             0.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of rows\n",
    "total_rows = titanic_data.shape[0]\n",
    "\n",
    "# Get the count of non-missing values for each column from describe\n",
    "counts_from_describe = titanic_data.describe().loc['count']\n",
    "\n",
    "# Compare and print discrepancies\n",
    "discrepancies = total_rows - counts_from_describe\n",
    "print(discrepancies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c50be817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least one missing value: 708\n"
     ]
    }
   ],
   "source": [
    "rows_with_missing = titanic_data.isnull().any(axis=1).sum()\n",
    "print(f\"Number of rows with at least one missing value: {rows_with_missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6220ef2",
   "metadata": {},
   "source": [
    "## Step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c05e7",
   "metadata": {},
   "source": [
    "### The discrepancies between the size of the dataset given by df.shape and what is reported by df.describe() occur because df.shape will count all the observations in the dataset no matter if there is a missing value or not, and df.describe() only count the observations that have no missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba1edd",
   "metadata": {},
   "source": [
    "## Step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533d1a3",
   "metadata": {},
   "source": [
    "### The difference between \"attribute\" and \"method\" is that \"attribute\" refers to a certain character of data which cannot be changed or edited, and \"method\" refers to a function that we can use to make changes to some data and take some actions like computing to acquire some consequences we want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bce998",
   "metadata": {},
   "source": [
    "## Step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf074c",
   "metadata": {},
   "source": [
    "### df.describe( ):\n",
    "\n",
    "    1. count: The number of non-missing data for each numerical column.\n",
    "    2. mean: The average value of each numerical column.\n",
    "    3. sta: The standard deviation of each numerical column.\n",
    "    4. min: The minimum value found in each numerical column.\n",
    "    5. 25%: The 25th percentile in each numerical column.\n",
    "    6. 50%: The 50th percentile in each numerical column.\n",
    "    7. 75%: The 75th percentile in each numerical column.\n",
    "    8. max: The maximum value found in each numerical column.\n",
    "\n",
    "    df.describe() will ignore missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700be18",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "### Q1: When I distribute a survey online and make a dataset out of the data I get, it has columns for name, date of birth, hobbies, and hours of sleep, but not all the respondents answered all the questions, resulting in some of the rows in my dataset missing some of the data, and I only want to analyze the data that is complete, then I can use df.dropna() instead of del df['col'].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f6c75",
   "metadata": {},
   "source": [
    "### Q2: When I analyzed the dataset I collected from a survey online, I noticed that almost no one answered the \"sleep hours\" question. Since this column has little valuable information, I decided to delete the whole columnthen, then I can use del df['col'] instead of df.dropna().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97b7d0",
   "metadata": {},
   "source": [
    "### Q3: Since there may be some columns with much missing data and little value to analyze, our main goal is to delete the columns. If we use df.dropna() first, it will delete all rows with missing data, which will cause unnecessary data loss. So it is important to apply del df['col'] before df.dropna() when both are used together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d380c31",
   "metadata": {},
   "source": [
    "### Q4: When removing all missing data, I will first use del df['col'] to delete all columns that have little valuable information, then I will use df.dropna() to delete the rest rows with missing data. Before applying the approach, the dataset will have multiple columns and rows with missing data, and after applying the approach, all the columns with missing data and the rest of the rows with missing data will be deleted, which means that there will be no missing data after applying the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c0874",
   "metadata": {},
   "source": [
    "## Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c92869",
   "metadata": {},
   "source": [
    "### Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a22bf22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url1 = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url1)\n",
    "\n",
    "print (df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7766eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count       mean        std   min   25%   50%   75%   max\n",
      "survived                                                           \n",
      "0         424.0  30.626179  14.172110  1.00  21.0  28.0  39.0  74.0\n",
      "1         290.0  28.343690  14.950952  0.42  19.0  28.0  36.0  80.0\n"
     ]
    }
   ],
   "source": [
    "grouped_description = df.groupby(\"survived\")[\"age\"].describe()\n",
    "\n",
    "print(grouped_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4619e1",
   "metadata": {},
   "source": [
    "### Q2: The difference exists because df.describe() counts all the non-missing values of the whole column, and something like df.groupby(\"col1\")[\"col2\"].describe() counts non-missing values from each divided group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc03ea",
   "metadata": {},
   "source": [
    "### Q3: It is easier to work in a ChatBot session to fix the errors. We can use ChatBot to fix the coding errors. Google provided me with a lot of websites which were not as easy as ChatBot to find solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5b555",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffc9f7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m titanic_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "titanic_data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fff87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "class                                                                      \n",
      "First   216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "Second  184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "Third   491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import the pandas library and assign it the alias 'pd'\n",
    "\n",
    "# Load the Titanic dataset from the provided URL\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Example of groupby and describe usage: Grouping by 'class' and describing 'fare'\n",
    "grouped_description = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "# Display the result\n",
    "print(grouped_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df84b35",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1309e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m titanic_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanics.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"titanics.csv\"\n",
    "titanic_data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d682b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "class                                                                      \n",
      "First   216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "Second  184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "Third   491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Example: Grouping by 'class' and describing 'fare'\n",
    "    grouped_description = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3e491",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e387daf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grouped_description \u001b[38;5;241m=\u001b[39m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_description)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_description = DF.groupby(\"survived\")[\"age\"].describe()\n",
    "\n",
    "print(grouped_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dd2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count       mean        std   min   25%   50%   75%   max\n",
      "survived                                                           \n",
      "0         424.0  30.626179  14.172110  1.00  21.0  28.0  39.0  74.0\n",
      "1         290.0  28.343690  14.950952  0.42  19.0  28.0  36.0  80.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)  # Make sure 'df' is in lowercase\n",
    "\n",
    "    # Example: Grouping by 'class' and describing 'fare'\n",
    "    grouped_description = df.groupby(\"survived\")[\"age\"].describe()\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337060ef",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f2ac95",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2363587796.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    titanic_data = pd.read_csv(url\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "titanic_data = pd.read_csv(url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b8069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "class                                                                      \n",
      "First   216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "Second  184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "Third   491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)  # Ensure parentheses are closed here\n",
    "\n",
    "    # Example: Grouping by 'class' and describing 'fare'\n",
    "    grouped_description = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")  # Make sure this parentheses is closed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d29cb8",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac588ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/1509504041.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"col2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "df.group_by(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45ad10e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "class                                                                      \n",
      "First   216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "Second  184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "Third   491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)  # Load the dataset into a DataFrame\n",
    "\n",
    "    # Correct method: Grouping by 'class' and describing 'fare'\n",
    "    grouped_description = df.groupby(\"class\")[\"fare\"].describe()  # Use 'groupby' without underscore\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b667cbb",
   "metadata": {},
   "source": [
    "### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e22a261",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: Age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m url1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url1)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[0;32m-> 1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: Age'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url1 = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url1)\n",
    "\n",
    "df.groupby(\"sex\")[\"Age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ba617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the DataFrame:\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Display column names to check their correctness\n",
    "    print(\"Available columns in the DataFrame:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Correct usage of 'age' instead of 'Age'\n",
    "    grouped_description = df.groupby(\"class\")[\"age\"].describe()\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. Please check the column name.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197f861",
   "metadata": {},
   "source": [
    "### 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a5a722",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msex\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m)[age]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sex' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupby(sex)[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a083eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[43mage\u001b[49m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'age' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupby(\"sex\")[age].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "195d1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the DataFrame:\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct URL to the Titanic dataset on Seaborn's GitHub repository\n",
    "url = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Read the dataset from the URL\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Display column names to check their correctness\n",
    "    print(\"Available columns in the DataFrame:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Correct usage: Enclose 'age' in quotes to reference the column\n",
    "    grouped_description = df.groupby(\"class\")[\"age\"].describe()\n",
    "\n",
    "    # Display the result\n",
    "    print(grouped_description)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: File not found. Please check the URL or your internet connection.\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. Please check the column name.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3017a",
   "metadata": {},
   "source": [
    "## Step 9:\n",
    "### Somewhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb522f32",
   "metadata": {},
   "source": [
    "## ChatBot Links and Summaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f3836",
   "metadata": {},
   "source": [
    "### 1. First dataset used from TUT: https://chatgpt.com/share/aa9bb2c8-5713-42c2-a02f-c7669c479a0f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961c496",
   "metadata": {},
   "source": [
    "### 2. Link:https://chatgpt.com/c/66e0ec5b-7b60-8003-94d1-3376120fe376"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf128c",
   "metadata": {},
   "source": [
    "### Summary: Here's a summary of our conversation:\n",
    "\n",
    "Missing Values in the Dataset:\n",
    "\n",
    "You learned how to identify missing values in a dataset using df.isnull().sum(), which shows the number of missing values in each column.\n",
    "Dataset Dimensions:\n",
    "\n",
    "You discovered how to check the number of rows and columns in the dataset using df.shape, which returns a tuple with the count of rows and columns.\n",
    "Observations vs. Variables:\n",
    "\n",
    "Observations: The individual data points or records in the dataset (each row represents a passenger in the Titanic dataset).\n",
    "Variables: The attributes or features of the observations (each column represents a different attribute, such as Age, Sex, etc.).\n",
    "Summarizing the Dataset:\n",
    "\n",
    "The simplest summary of the dataset can be achieved using df.describe(), which provides key statistical measures (like count, mean, std, min, max, and percentiles) for numerical columns.\n",
    "Additional methods for summarization include df.info() (for data types and missing values), df.head() (for viewing the first few rows), value_counts() (for categorical columns), and correlation analysis.\n",
    "Attributes vs. Methods:\n",
    "\n",
    "Attributes (e.g., df.shape) provide stored information about an object and do not require parentheses.\n",
    "Methods (e.g., df.describe()) perform actions or calculations on the data and require parentheses to execute.\n",
    "What df.describe() Provides:\n",
    "\n",
    "df.describe() gives a statistical summary of numerical columns by default, showing count, mean, standard deviation, min, max, and percentiles. It can also be customized to include all columns (numerical and categorical).\n",
    "Handling Missing Data in df.describe():\n",
    "\n",
    "Missing data is automatically excluded from calculations in df.describe(). The count reflects the number of non-missing values, while other statistics are calculated based only on non-missing data.\n",
    "Checking for Discrepancies:\n",
    "\n",
    "To check for discrepancies between the size of the dataset (from df.shape) and the summary provided by df.describe(), you can compare the counts and look for missing data in columns.\n",
    "This summary captures the key points from our discussion about analyzing and summarizing data using pandas in Python, focusing on understanding attributes, methods, and handling missing data in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4d94b",
   "metadata": {},
   "source": [
    "### 3. Link:https://chatgpt.com/c/66e211f3-9e58-8003-a608-1f271984d25b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a3f31",
   "metadata": {},
   "source": [
    "### Summary: Here's a summary of our conversation:\n",
    "\n",
    "Use Cases for df.dropna() vs. del df['col']:\n",
    "\n",
    "df.dropna() is preferred when you want to remove rows or columns with missing values. This is useful for data cleaning, where you need to keep only complete records.\n",
    "del df['col'] is preferred when you want to remove an entire column that is irrelevant or not needed for your analysis, regardless of whether it has missing values.\n",
    "Importance of Using del df['col'] Before df.dropna():\n",
    "\n",
    "Applying del df['col'] before df.dropna() prevents the unintended removal of rows due to missing values in columns you plan to delete anyway. It avoids unnecessary data loss and improves efficiency by focusing only on the columns that are relevant to your analysis.\n",
    "Removing All Missing Data from a Dataset:\n",
    "\n",
    "You can combine del df['col'] and df.dropna() to remove all missing data:\n",
    "First, use del df['col'] to remove irrelevant columns.\n",
    "Then, apply df.dropna() to remove rows or columns that contain any remaining missing values.\n",
    "This approach ensures that only necessary data is kept and all missing values are removed effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3cfbc",
   "metadata": {},
   "source": [
    "### 4. Link: https://chatgpt.com/c/66e220b8-3214-8003-b4ba-27524370d842"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123a720",
   "metadata": {},
   "source": [
    "### Summary: Here's a summary of our conversation:\n",
    "\n",
    "Initial Request: You wanted to perform some initial summary analyses on the Titanic dataset and needed help understanding specific parts of your code.\n",
    "\n",
    "Understanding groupby().describe():\n",
    "\n",
    "We discussed what the df.groupby(\"col1\")[\"col2\"].describe() code does. It groups the DataFrame by the unique values in col1 and computes descriptive statistics (like count, mean, std, min, max, etc.) for col2 within each group.\n",
    "Count Differences Explanation:\n",
    "\n",
    "I explained why the count values might differ between df.describe() and df.groupby(\"col1\")[\"col2\"].describe(). The former computes statistics for the entire column, while the latter calculates statistics for each group separately.\n",
    "Common Errors and Fixes:\n",
    "\n",
    "NameError: name 'pd' is not defined: This error was due to the pandas library not being imported. The solution was to include import pandas as pd at the beginning of your code.\n",
    "HTTPError: HTTP Error 404: Not Found: The URL used to load the Titanic dataset was incorrect, and I provided the correct URL.\n",
    "FileNotFoundError: Explained that this error could occur due to network issues or a typo in the file path or URL.\n",
    "NameError: name 'DF' is not defined: This error happened because Python is case-sensitive, and df should be lowercase.\n",
    "SyntaxError: '(' was never closed: Explained that this error is due to a missing closing parenthesis and provided guidance on ensuring all parentheses are balanced.\n",
    "AttributeError: 'DataFrame' object has no attribute 'group_by': This was caused by using the incorrect method name group_by instead of groupby.\n",
    "KeyError: 'Column not found: Age': The error occurred due to using an incorrect column name (Age instead of age). We discussed how to check the DataFrame’s columns using df.columns.\n",
    "NameError: name 'age' is not defined: This error was due to not using quotes around the column name age. I clarified the need to enclose column names in quotes.\n",
    "Final Guidance: I provided detailed explanations and corrected code snippets for each of these errors to help you resolve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8fbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
